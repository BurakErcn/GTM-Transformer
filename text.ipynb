{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from transformers import pipeline\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(args):\n",
    "    print(args)\n",
    "    # Seeds for reproducibility (By default we use the number 21)\n",
    "\n",
    "    # Load sales data\n",
    "    data_folder = \"./dataset/\"\n",
    "\n",
    "    # Load category and color encodings\n",
    "    cat_dict = torch.load(Path(data_folder + 'category_labels.pt'))\n",
    "    col_dict = torch.load(Path(data_folder + 'color_labels.pt'))\n",
    "    fab_dict = torch.load(Path(data_folder + 'fabric_labels.pt'))\n",
    "\n",
    "    print(cat_dict)\n",
    "    print(col_dict)\n",
    "    print(fab_dict)\n",
    "\n",
    "    return cat_dict, col_dict, fab_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "{'capris': 0, 'cloak': 1, 'culottes': 2, 'doll dress': 3, 'drop sleeve': 4, 'gitana skirt': 5, 'kimono dress': 6, 'long cardigan': 7, 'long coat': 8, 'long dress': 9, 'long duster': 10, 'long skirt': 11, 'long sleeve': 12, 'longuette skirt': 13, 'maxi': 14, 'medium cardigan': 15, 'medium coat': 16, 'medium duster': 17, 'midi skirt': 18, 'miniskirt': 19, 'patterned': 20, 'printed': 21, 'sheath dress': 22, 'shirt dress': 23, 'short cardigan': 24, 'short coat': 25, 'short sleeves': 26, 'shorts': 27, 'sleeveless': 28, 'solid colours': 29, 'tracksuit': 30, 'trapeze dress': 31}\n",
      "{'yellow': 0, 'brown': 1, 'blue': 2, 'grey': 3, 'green': 4, 'black': 5, 'red': 6, 'white': 7, 'orange': 8, 'violet': 9}\n",
      "{'acrylic': 0, 'scuba crepe': 49, 'tulle': 55, 'angora': 1, 'faux leather': 14, 'georgette': 21, 'lurex': 30, 'nice': 38, 'crepe': 9, 'satin cotton': 47, 'silky satin': 51, 'fur': 20, 'matte jersey': 33, 'plisse': 43, 'velvet': 56, 'lace': 27, 'cotton': 8, 'piquet': 42, 'plush': 45, 'bengaline': 2, 'jacquard': 26, 'frise': 19, 'technical': 53, 'cady': 3, 'dark jeans': 11, 'light jeans': 28, 'ity': 25, 'plumetis': 44, 'polyviscous': 46, 'dainetto': 10, 'webbing': 58, 'foam rubber': 18, 'heavy jeans': 23, 'chanel': 5, 'marocain': 32, 'macrame': 31, 'embossed': 13, 'nylon': 39, 'tencel': 54, 'paillettes': 41, 'chambree': 4, 'chine crepe': 6, 'linen': 29, 'muslin cotton or silk': 36, 'tactel': 52, 'viscose twill': 57, 'cloth': 7, 'mohair': 35, 'mutton': 37, 'scottish': 48, 'milano stitch': 34, 'devore': 12, 'hron': 24, 'ottoman': 40, 'fluid': 16, 'flamed': 15, 'fluid polyviscous': 17, 'shiny jersey': 50, 'goose': 22}\n"
     ]
    }
   ],
   "source": [
    "cat_dict, col_dict, fab_dict = run(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextEmbedder(nn.Module):\n",
    "    def __init__(self, embedding_dim, cat_dict, col_dict, fab_dict, gpu_num):\n",
    "        super().__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.cat_dict = {v: k for k, v in cat_dict.items()}\n",
    "        self.col_dict = {v: k for k, v in col_dict.items()}\n",
    "        self.fab_dict = {v: k for k, v in fab_dict.items()}\n",
    "        self.word_embedder = pipeline('feature-extraction', model='bert-base-uncased')\n",
    "        self.fc = nn.Linear(768, embedding_dim)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.gpu_num = gpu_num\n",
    "\n",
    "    def forward(self, category, color, fabric):\n",
    "        textual_description = [self.col_dict[color.detach().cpu().numpy().tolist()[i]] + ' ' \\\n",
    "                + self.fab_dict[fabric.detach().cpu().numpy().tolist()[i]] + ' ' \\\n",
    "                + self.cat_dict[category.detach().cpu().numpy().tolist()[i]] for i in range(len(category))]\n",
    "        # Hafta ve maÄŸaza idsi eklenmeli.\n",
    "\n",
    "        # Use BERT to extract features\n",
    "        word_embeddings = self.word_embedder(textual_description)\n",
    "\n",
    "        # BERT gives us embeddings for [CLS] ..  [EOS], which is why we only average the embeddings in the range [1:-1] \n",
    "        # We're not fine tuning BERT and we don't want the noise coming from [CLS] or [EOS]\n",
    "        word_embeddings = [torch.FloatTensor(x[0][1:-1]).mean(axis=0) for x in word_embeddings] \n",
    "        word_embeddings = torch.stack(word_embeddings)\n",
    "        \n",
    "        # Embed to our embedding space\n",
    "        word_embeddings = self.dropout(self.fc(word_embeddings))\n",
    "\n",
    "        return word_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_encoder = TextEmbedder(32, cat_dict, col_dict, fab_dict, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.data_multitrends import ZeroShotDataset\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(Path(\"./dataset/\" + 'train.csv'), parse_dates=['release_date'])\n",
    "test_df = pd.read_csv(Path(\"./dataset/\" + 'test.csv'), parse_dates=['release_date'])\n",
    "\n",
    "gtrends = pd.read_csv(Path(\"./dataset/\" + 'gtrends.csv'), index_col=[0], parse_dates=True)\n",
    "\n",
    "train_loader = ZeroShotDataset(train_df, Path(\"./dataset/\" + '/images'), gtrends, cat_dict, col_dict,\n",
    "                                fab_dict, 52).get_loader(batch_size=128, train=True)\n",
    "test_loader = ZeroShotDataset(test_df, Path(\"./dataset/\" + '/images'), gtrends, cat_dict, col_dict,\n",
    "                                fab_dict, 52).get_loader(batch_size=1, train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset():\n",
    "    def __init__(self, data_df, cat_dict, col_dict, fab_dict, trend_len):\n",
    "        self.data_df = data_df\n",
    "        self.cat_dict = cat_dict\n",
    "        self.col_dict = col_dict\n",
    "        self.fab_dict = fab_dict\n",
    "        self.trend_len = trend_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data_df.iloc[idx, :]\n",
    "\n",
    "    def preprocess_data(self):\n",
    "        data = self.data_df\n",
    "        # Remove non-numerical information\n",
    "        data.drop(['external_code', 'season', 'release_date'], axis=1, inplace=True) # Arrange drops!\n",
    "\n",
    "        # Create tensors for each part of the input/output\n",
    "        item_sales, temporal_features = torch.FloatTensor(data.iloc[:, :12].values), torch.FloatTensor( # Arrange column indexes!\n",
    "            data.iloc[:, 13:17].values) # Arrange column indexes!\n",
    "        categories, colors, fabrics = [self.cat_dict[val] for val in data.iloc[:].category.values], \\\n",
    "                                       [self.col_dict[val] for val in data.iloc[:].color.values], \\\n",
    "                                       [self.fab_dict[val] for val in data.iloc[:].fabric.values]\n",
    "\n",
    "        \n",
    "        categories, colors, fabrics = torch.LongTensor(categories), torch.LongTensor(colors), torch.LongTensor(fabrics)\n",
    "\n",
    "        return TensorDataset(item_sales, categories, colors, fabrics, temporal_features)\n",
    "\n",
    "    def get_loader(self, batch_size, train=True):\n",
    "        print('Starting dataset creation process...')\n",
    "        data_with_gtrends = self.preprocess_data()\n",
    "        data_loader = None\n",
    "        if train:\n",
    "            data_loader = DataLoader(data_with_gtrends, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "        else:\n",
    "            data_loader = DataLoader(data_with_gtrends, batch_size=1, shuffle=False, num_workers=4)\n",
    "        print('Done.')\n",
    "\n",
    "        return data_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting dataset creation process...\n",
      "Done.\n",
      "Starting dataset creation process...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv(Path(\"./dataset/\" + 'train.csv'), parse_dates=['release_date'])\n",
    "test_df = pd.read_csv(Path(\"./dataset/\" + 'test.csv'), parse_dates=['release_date'])\n",
    "\n",
    "train_loader = TextDataset(data_df=train_df, cat_dict=cat_dict, col_dict=col_dict,\n",
    "                                fab_dict=fab_dict, trend_len=52).get_loader(batch_size=128, train=True)\n",
    "test_loader = TextDataset(test_df, cat_dict, col_dict,\n",
    "                                fab_dict, 52).get_loader(batch_size=1, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Epoch\n",
    "for data in train_loader:\n",
    "    item_sales, category, color, fabric, temporal_features = data \n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_encoding = text_encoder(category, color, fabric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text_encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text_encoding[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gtm-transformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
